#!/usr/bin/env python
### dcfetch.py
##
## Copyright (c) 2018 Matthew Love <matthew.love@colorado.edu>
##
## Permission is hereby granted, free of charge, to any person obtaining a copy 
## of this software and associated documentation files (the "Software"), to deal 
## in the Software without restriction, including without limitation the rights 
## to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies 
## of the Software, and to permit persons to whom the Software is furnished to do so, 
## subject to the following conditions:
##
## The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
##
## THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, 
## INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR 
## PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE 
## FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, 
## ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
##
### Description:
##
## Access elevation data from NOAA's Digital Coast.
##
## Examples:
##
## * Return a list of lidar datasets that fall within a given region and occured after 2012: 
## dcfetch.py -R -90.75/-88.1/28.7/31.25 --filter "Year > 2012 and Datatype = 'lidar'" --list-only --index
##
## * Download all the lidar tiles that fall within the polygons in the shapefile tileset 'input_ply.shp'
## dcfetch.py -R input_ply.shp -f "Datatype = 'lidar'"
##
## * Save the search results to a file to fetch later with wget:
## dcfetch.py -R input_ply.shp -f "Datatype = 'lidar'" -l > input_ply.urls
## wget -i input_ply.urls
### Code:

import os
import sys
import ftplib
import urllib2
from xml.dom import minidom
import csv

try:
    import osgeo.ogr as ogr
except ImportError:
    try:
        import ogr
    except ImportError:
       sys.exit('''
fetch: Sorry: You must have the Python GDAL/OGR bindings for OGR support,
Get them here: http://trac.osgeo.org/gdal/wiki/GdalOgrInPython''')

_version = '0.0.6'

_license = """
version %s
Copyright (c) 2018 Matthew Love <matthew.love@colorado.edu>

Permission is hereby granted, free of charge, to any person obtaining a copy 
of this software and associated documentation files (the "Software"), to deal 
in the Software without restriction, including without limitation the rights 
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies 
of the Software, and to permit persons to whom the Software is furnished to do so, 
subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, 
INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR 
PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE 
FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, 
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
    """ %(_version)

_usage = """
Fetch Data from NOAA's Digital Coast

usage: dcfetch.py [ -hflRuvx [ args ] ]

Options:
  -R, --region\t\tSpecifies the desired region to search; this can either be a GMT-style region ( -R xmin/xmax/ymin/ymax )
\t\t\tor an OGR-compatible vector file with regional polygons. If a vector file is supplied, 
\t\t\twill search each region found in the vector file.
  -f, --filter\t\tSQL style attribute filter; Current field names are : Name, ID, Data, Year, Datatype

  -l, --list-only\tOutput a list of urls for all tiles that pass all filtering
  -x, --index\t\tOutput a list of datasets that pass all filtering

  --update\t\tScan for new datasets and update the vector
  --help\t\tPrint the usage text
  --version\t\tPrint the version information

Examples:

dcfetch.py -R -90.75/-88.1/28.7/31.25 --filter "Year > 2012 and Datatype = 'lidar'" --list-only --index

dcfetch.py v.%s
""" %(_version)

_dc_ftp_url_full = "ftp://coast.noaa.gov"
_dc_ftp_url = "coast.noaa.gov"
_dc_dav_id = "https://coast.noaa.gov/dataviewer/#/lidar/search/where:ID="
_dc_ftp_dirs = ['raster2', 'lidar1_z', 'lidar2_z']
#_dc_ftp_dirs = ['raster2']
_dc_ftp_subdirs = ['geoid12a', 'geoid12b', 'elevation']
#_dc_ftp_subdirs = ['elevation']

fetchdata = "@fetchdir@"
if "/" not in fetchdata:
    fetchdata = ""

def _bounds2geom(bounds):
    b1 = [[bounds[2], bounds[0]],
          [bounds[2], bounds[1]],
          [bounds[3], bounds[1]],
          [bounds[3], bounds[0]],
          [bounds[2], bounds[0]]]
    return ogr.CreateGeometryFromWkt(_create_polygon(b1))

def _create_polygon(coords):          
    ring = ogr.Geometry(ogr.wkbLinearRing)
    for coord in coords:
        ring.AddPoint(coord[1], coord[0])
    poly = ogr.Geometry(ogr.wkbPolygon)
    poly.AddGeometry(ring)
    return poly.ExportToWkt()

class dc_bounds:
    def __init__(self, extent=None, index=False):
        self.dcftp = dc_ftp()
        self._index = index
        self.surveys = []
        self._results = []
        if os.path.exists(fetchdata + "digital_coast.gmt"): self.want_update = True
        else: self.want_update = False
        #if extent is not None: self._boundsGeom = _bounds2geom(extent)
        self._boundsGeom = extent

    def fetch_url(self, url):
        print "fetching " + url

        _dirname = "./" + os.path.dirname(url.split(".gov")[1])
        
        if not os.path.exists(_dirname):
            os.makedirs(_dirname)
 
        with open(_dirname + "/" + os.path.basename(url), "wb") as local_file:
            f = urllib2.urlopen(url)
            local_file.write(f.read())
        f.close()

    def _fetch_res(self, url):
        response = urllib2.urlopen(url)
        results = response.read()
        response.close()
        return results

    def _fetch_xml(self, url):
        return minidom.parseString(self._fetch_res(url))

    def _fetch_csv(self, url):
        csv_results = csv.reader(self._fetch_res(url).split('\n'), delimiter=",")
        return list(csv_results)
        
    def _get_xml_extents(self, xml):
        bounding = xml.getElementsByTagName("bounding")
        try:
            for node in bounding:
                wl = node.getElementsByTagName("westbc")[0].firstChild.nodeValue
                el = node.getElementsByTagName("eastbc")[0].firstChild.nodeValue
                sl = node.getElementsByTagName("southbc")[0].firstChild.nodeValue
                nl = node.getElementsByTagName("northbc")[0].firstChild.nodeValue
            return map(float, [wl,el,sl,nl])
        except:
            return False

    def _get_xml_title(self, xml):
        return xml.getElementsByTagName("title")[0].firstChild.nodeValue

    def _get_xml_pubdate(self, xml):
        date_xml = xml.getElementsByTagName("enddate")
        if not date_xml: date_xml = xml.getElementsByTagName("caldate")
        try: return date_xml[0].firstChild.nodeValue
        except: return False

    def _get_dc_link(self, xml):
        date_xml = xml.getElementsByTagName("enddate")
        if not date_xml: date_xml = xml.getElementsByTagName("caldate")
        try: out_date = date_xml[0].firstChild.nodeValue
        except: out_date = 0000
        return out_date

    def _get_xml_metadate(self, xml):
        return xml.getElementsByTagName("metd")[0].firstChild.nodeValue
        
    def _append_list(self, dataset_id, survey_xml, survey_url, survey_dtype):
        this_bounding = self._get_xml_extents(survey_xml)
        this_title = self._get_xml_title(survey_xml)
        this_date = self._get_xml_pubdate(survey_xml)
        if this_date: this_date = this_date[:4]
        else: this_date = '1900'
        s_entry = [dataset_id, this_title, this_bounding, survey_url, this_date, survey_dtype]
        if this_bounding:
            print("dcfetch: appending %s (%s): %s - %s" %(s_entry[0], s_entry[5], s_entry[1], s_entry[4]))
            self.surveys.append(s_entry)

    def _update_directory(self, s_dtype):
        if self.want_update:
            gmt2 = ogr.GetDriverByName('GMT').Open(fetchdata + "digital_coast.gmt", 0)
            layer = gmt2.GetLayer()
        else: layer = []
        pdir = self.dcftp.ftp.pwd()
        if s_dtype == 'lidar':
            self.dcftp.ftp.cwd("data")
        data_dir = self.dcftp.ftp.pwd()
        datalist = self.dcftp.ftp.nlst()
        for dataset in datalist:
            try: did = int(dataset.split("_")[-1])
            except: did = None
            if did:
                if self.want_update:
                    layer.SetAttributeFilter("ID = %s" %did)
                if len(layer) == 0:
                    self.dcftp.ftp.cwd(dataset)
                    dc_files = self.dcftp.ftp.nlst()
                    for dc_file in dc_files:
                        if "metadata.xml" in dc_file or 'met.xml' in dc_file:
                            xml_url = _dc_ftp_url_full + self.dcftp.ftp.pwd() + "/" + dc_file
                            dc_xml = self._fetch_xml(xml_url)
                            self._append_list(did, dc_xml, _dc_ftp_url_full + self.dcftp.ftp.pwd() + "/", s_dtype)
                            break
            self.dcftp.ftp.cwd(data_dir)
        self.dcftp.ftp.cwd(pdir)
        gmt2 = layer = None

    def _update(self):
        self.filelist = self.dcftp.ftp.nlst()
        print('dcfetch: scanning for surveys')
        for i in _dc_ftp_dirs:
            s_dtype = i.split("1")[0].split("2")[0]
            self.dcftp.ftp.cwd(i)
            sub_dir = self.dcftp.ftp.pwd()
            for j in _dc_ftp_subdirs:
                try:
                    self.dcftp.ftp.cwd(j)
                    self._update_directory(s_dtype)
                except: pass
                self.dcftp.ftp.cwd(sub_dir)
            self.dcftp._home()
        if len(self.surveys) > 0:
            self._update_gmt()
        print('dcfetch: all up-to-date')

    def _update_gmt(self):
        if self.want_update:
            gmt1 = ogr.GetDriverByName('GMT').Open(fetchdata + "digital_coast.gmt", 1)
            layer = gmt1.GetLayer()
        else:
            ds = ogr.GetDriverByName('GMT').CreateDataSource(fetchdata + "digital_coast.gmt")
            layer = ds.CreateLayer("digital_coast", None, ogr.wkbPolygon)
            layer.CreateField(ogr.FieldDefn('Name', ogr.OFTString))
            layer.CreateField(ogr.FieldDefn('ID', ogr.OFTInteger))
            layer.CreateField(ogr.FieldDefn('Data', ogr.OFTString))
            layer.CreateField(ogr.FieldDefn('Year', ogr.OFTInteger))
            layer.CreateField(ogr.FieldDefn('Datatype', ogr.OFTString))
            self.want_update = True
        for feature in layer:
            layer.SetFeature(feature)
        for i in self.surveys:
            self._add_feature_gmt(layer, i)
        gmt1 = layer = None

    def _add_feature_gmt(self, ogr_layer, survey):
        try:
            geom = _bounds2geom(survey[2])
            feat = ogr.Feature(ogr_layer.GetLayerDefn())
            feat.SetGeometry(geom)
            feat.SetField("Name", str(survey[1]))
            feat.SetField("ID", int(survey[0]))
            feat.SetField("Data", str(survey[3]))
            feat.SetField("Year", int(survey[4]))
            feat.SetField("Datatype", str(survey[5]))
            ogr_layer.CreateFeature(feat)
        except: pass
        feat = geom = None

    def search_raster_dataset(self, surv_url):
        self.dcftp.ftp.cwd(surv_url.split(".gov")[1])
        datalist = self.dcftp.ftp.nlst()
        has_shp = False
        shps = []
        for dc_file in datalist:
            if "0tileindex" in dc_file:
                #if ".shp" in dc_file or ".shx" in dc_file or ".dbf" in dc_file or ".prj" in dc_file:
                shps.append(dc_file)
                has_shp = True

        if has_shp:
            for i in shps:
                self.dcftp.fetch_file(i, False)
                if '.shp' in i: tile_shp = i
                else: tile_shp = None
            if tile_shp:
                shp1 = ogr.Open(tile_shp)
                slay1 = shp1.GetLayer(0)
                for sf1 in slay1:
                    geom = sf1.GetGeometryRef()
                    if self._boundsGeom.Intersects(geom):
                        tile_url = sf1.GetField("URL").strip()
                        self._results.append(tile_url)
                shp1 = slay1 = None
            for i in shps:
                os.remove(i)

    def search_lidar_dataset(self, surv_url):
        self.dcftp.ftp.cwd(surv_url.split(".gov")[1])
        datalist = self.dcftp.ftp.nlst()
        for dc_file in datalist:
            ## Lidar (each lidar directory has a .csv file with the name and boundary of each tile)
            if ".csv" in dc_file:
                dc_csv = self._fetch_csv(surv_url + dc_file)
                for tile in dc_csv:
                    try:
                        tb = [float(tile[1]),float(tile[2]),float(tile[3]),float(tile[4])]
                        tile_geom = _bounds2geom(tb)
                        if tile_geom.Intersects(self._boundsGeom):
                            self._results.append(surv_url + tile[0])
                    except: pass
                break
                
    def search_gmt(self, filters=[]):
        gmt1 = ogr.Open(fetchdata + "digital_coast.gmt")
        layer = gmt1.GetLayer(0)
        for filt in filters:
            layer.SetAttributeFilter("%s" %filt)
        for feature1 in layer:
            geom = feature1.GetGeometryRef()
            if self._boundsGeom.Intersects(geom):
                surv_url = feature1.GetField("Data")
                if self._index:
                    print("%s (%s): %s (%s) - %s" %(feature1.GetField("ID"),feature1.GetField("Datatype"),feature1.GetField("Name"),feature1.GetField("Year"),feature1.GetField("Data")))
                    #print("Name: %s" %(feature1.GetField("Name")))
                    #print("ID: %s" %(feature1.GetField("ID")))
                    #print("Year: %s" %(feature1.GetField("Year")))
                    #print("Datatype: %s" %(feature1.GetField("Datatype")))
                    #print("Data: %s" %(feature1.GetField("Data")))
                    #print("%s (%s): %s%s" %(feature1.GetField("ID"),feature1.GetField("Datatype"),_dc_dav_id, feature1.GetField("ID")))
                    #print("%s (%s): %s" %(feature1.GetField("ID"),feature1.GetField("Datatype"),feature1.GetField("Data")))
                    #print("----")
                    pass
                else:
                    if feature1.GetField("Datatype") == "lidar":
                        self.search_lidar_dataset(surv_url) 
                    elif feature1.GetField("Datatype") == "raster":
                        self.search_raster_dataset(surv_url)

    def print_results(self):
        for row in self._results:
            print(row)

    def fetch_results(self):
        for row in self._results:
            self.fetch_url(row)

class dc_ftp:
    def __init__(self):
        self.ftp = ftplib.FTP(_dc_ftp_url)
        self.ftp.login()
        self.ftp.cwd("/pub/DigitalCoast")
        self.parent = self.ftp.pwd()

    def _home(self):
        self.ftp.cwd(self.parent)

    def _list(self):
        self.ftp.retrlines("LIST")

    def fetch_file(self, filename, full=True):
        if full:
            dirname = "." + self.ftp.pwd()
            if not os.path.exists(dirname):
                os.makedirs(dirname)
        else: dirname = "."
            
        with open(dirname + "/" + os.path.basename(filename), "wb") as local_file:
            self.ftp.retrbinary('RETR %s' % filename, local_file.write)

if __name__ == '__main__':
    
    extent = None
    extents = []
    poly = False
    want_list = False
    want_update = False
    want_index = False
    f = []

    i = 1
    while i < len(sys.argv):
        arg = sys.argv[i]

        if arg == '-R' or arg == '--region':
            try:
                extent = map(float, sys.argv[i+1].split("/"))
                poly = False
            except:
                extent = sys.argv[i+1]
                poly = True
            i = i + 1

        elif arg == '--list-only' or arg == '-l':
            want_list = True

        elif arg == '--update' or arg == '-u':
            want_update = True
            
        elif arg == '--filter' or arg == '-f':
            f.append(sys.argv[i+1])
            i = i + 1

        elif arg == '--index' or arg == '-x':
            want_index = True
            
        elif arg == '--help' or arg == '-h':
            print(_usage)
            sys.exit(1)

        elif arg == '--version' or arg == '-v':
            print('dcfetch.py v.%s' %(_version))
            print(_license)
            sys.exit(1)

        else:
            print(_usage)
            sys.exit(1)

        i = i + 1

    if extent is None and len(f) > 0:
        extent = [-180,180,-90,90]

    if extent is None and want_update is False:
        print(_usage)
        sys.exit(1)

    if poly:
        _poly = ogr.Open(extent)
        _player = _poly.GetLayer(0)
        for pf in _player:
            _pgeom = pf.GetGeometryRef()

            dcb = dc_bounds(_pgeom, want_index)
            if want_update:
                dcb._update()
            else: 
                if want_index: 
                    print("%s: %s" %(_player.GetLayerDefn().GetName(), _pgeom))
                dcb.search_gmt(f)
                if want_index: 
                    print("---")
                if want_list: dcb.print_results()
                else: dcb.fetch_results()
        _poly = _player = None
    else:
        dcb = dc_bounds(_bounds2geom(extent), want_index)
        if want_update:
            dcb._update()
        else: 
            dcb.search_gmt(f)
            if want_list: dcb.print_results()
            else: dcb.fetch_results()

    #--

### End
## how many tiles selected in -x view
## make a fetch-data script in dem
